<!doctype html>
<html lang="en">

	<head>
		<meta charset="utf-8">

		<title>Re-identification of People in Outdoor Surveillance Videos</title>

		<meta name="description" content="A framework for easily creating beautiful presentations using HTML">
		<meta name="author" content="Hakim El Hattab">

		<meta name="apple-mobile-web-app-capable" content="yes">
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/sky.css" id="theme">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="css/zenburn.css">
		
		<!-- custom -->
		<link rel="stylesheet" href="css/custom.css">

	

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>

		<!--[if lt IE 9]>
		<script src="js/html5shiv.js"></script>
		<![endif]-->
	</head>

	<body>

		<div class="reveal">

			<!-- Any section element inside of this container is displayed as a slide -->
			<div class="slides">
				
				<section id="title">
					<h3>Re-identification of People in Outdoor Surveillance Videos</h3>
					<blockquote cite="" style= "font-size: x-large;margin-top: 50px;margin-bottom: 50px;">
						&ldquo;
						The system is capable of reidentify a given (query) person. <br/>
						The process annotate videos detecting pedestrian using a simple HOG detector 
						and than with a "Siamese Network" try to reidentify people by similarity ranking.
						 &rdquo;
					</blockquote>
					<p style= "font-size: x-large;">
						<small>Created by <a href="http://hakim.se">Mattia Chiarini</a> / <a href="https://github.com/matitaweb/mumet2017_internal_project">@mattiachiarini</a></small>
					</p>
					<hr style="clear:both"/>
					<p style= "font-size: x-small;">Press the <em>S</em> to SPEAKER VIEW<br/>Press <strong>ESC</strong> to enter the slide overview.</p>
				</section>
				
				<section id="demo">
					<div>
						<h5>First some results...</h5>
						<p style="font-size:large;">
							A simple surveilance video, for every reidentified pedestrian are drawn a bounding with the name and the identity propability...
						</p>
					</div>
					<iframe height="500px" width="800px" src="https://www.youtube.com/embed/RipYW9D15fs" frameborder="0" allowfullscreen></iframe>

				</section>

				<section id="assumptions">
					<h6>Some Project Assumptions.... </h6>
					<p style="clear: both;padding-top: 10px;">
						<div style="text-align: left">
							<img style="float: left; margin: 0px 15px 15px 0px;" src="./img/surveillance-video-camera.png" width="80" />
							Static camera or simple move as pan or tilt.
						</div>
					</p> 
							
					<p style="clear: both;padding-top: 10px;">
						<div style="text-align: left">
						<img style="float: left; margin: 0px 15px 15px 0px;" src="./img/walking.png" width="80" />
						We have a set of pedestrian image for the reidentification.
						</div>
					</p>
					<p style="clear: both;padding-top: 10px;">
						<div style="text-align: left">
						<img style="float: left; margin: 0px 15px 15px 0px;" src="./img/sun.png" width="80" />
						Good light condition
						</div>
					</p>
					<p style="clear: both;padding-top: 10px;">
						<div style="text-align: left">
						<img style="float: left; margin: 0px 15px 15px 0px;" src="./img/no-dogs.png" width="80" />
						No crowd or animals and moving objects.
						</div>
					</p>
					
				</section>

				<!-- Example of nested vertical slides -->
				<section id="architecture">
					
					<section id="architecture_main">
						<h6>Architecture</h6>
						<div>
						<div class="col_left_1_3" style='font-size: large;'>
							<table>
							<tbody>
								<tr>
									<td>
										<p>
											<b>Detection phase</b><br/>
											Simple HOG detection producing croped people image and frame annotation.
										</p>
									</td>
								</tr>
								<tr>
									<td>
										<p>
											<b>ReID phase</b><br/>
											A Trained Siamese Network try to reidentify pedestrian searching in a identity DB.<br/>
											The output is a similarity rank for everty peope to reidentify.
										</p>
									</td>
								</tr>
								<tr>
									<td>
										<p>
											<b>Video annotation phase</b><br/>
											Adding to the video reidentificatoin info.
										</p>
									</td>
								</tr>
							</tbody>
						</table>
						</div>
						
						<div class="col_right_2_3">
							<img src="img/architettura_soluzione.png"></img>
						</div>
						</div>
						<hr style="clear:both">
						<p style= "font-size: large;">
						Go deeper!
						</p>
					</section>
					
					<section id="architecture_pedestrian_detect">
						<h6>Pedestrian Detection</h6>
						<div>
						<div class="col_left_2_3">
							<p style="font-size: large;"><B>Built-in feature used:</B></p>
							<p style="font-size: small;"><span style="font-style: italic;font-family: monospace;">Python 3, OpenCV 3.2, Panda 0.20</span></p>
							<h6>Strenght</h6>
							<p style="font-size:large">
								1. Easy to implement/deploy.<br/>
								2. No training required.<br/>
								3. It work also if camera is moving.
							</p>
							<h6>Weakness</h6>
							<p style="font-size:large">
								1. Not always reliable with false positive.<br/>
								2. Missings when people figure is not complete.<br/>
								3. Requires tuning for every scenarios.<br/>
								4. Not so Fast, speedup requires a trade of with accuracy. <br/>
							</p>
						</div>
						
						<div class="col_right_1_3">
							<img src="img/detection_phase.png"></img>
						</div>
						</div>
						
						<hr style="clear:both">
						<p style= "font-size: x-large;">
						<small><a href="https://web.stanford.edu/class/cs231a/prev_projects_2016/pedestrian-detection-tracking.pdf">web.stanford.edu/class/cs231a/prev_projects_2016/pedestrian-detection-tracking.pdf</a><br/>
						<a href="http://www.pyimagesearch.com/2015/11/16/hog-detectmultiscale-parameters-explained/">hog-detectmultiscale-parameters-explained</a><br/>
						<a href="https://www.pyimagesearch.com/2014/11/10/histogram-oriented-gradients-object-detection/">histogram-oriented-gradients-object-detection</a>
						</small>
						</p>
					</section>
					
					<section id="architecture_hog_detection_details">
						<h6>Implementations details</h6>
						
						<div>
						<div class="col_left_1_3">
							<img src="img/hog_pedestrian_detail.jpg" height="550px"></img>
						</div>
						
						<div class="col_right_2_3" style="font-size: 36px; text-align: left;">
					
						<p style="font-size: small;margin-bottom: 0px;">Hog detection part...</p>
						<pre style="font-size: small; line-height: normal; margin-top: 0px;">
							<code class="hljs python" data-trim contenteditable>
# initialize the HOG descriptor/person detector
import cv2
hog = cv2.HOGDescriptor()
hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())
(rects, weights) = hog.detectMultiScale(frame_gray, winStride=(2,2), padding=(4,4), scale=1.05, hitThreshold=1, finalThreshold=2.0, useMeanshiftGrouping=False)
							</code>
						</pre>
						<p style="font-size: small;margin-bottom: 0px;">
							Non maxima suppression...
						</p>
						<pre style="font-size: small; line-height: normal; margin-top: 0px;">
							<code class="hljs" data-trim contenteditable>
# non max suppression
from imutils.object_detection import non_max_suppression
pick = non_max_suppression(rects, probs=None, overlapThresh=overlap_thresh)
							</code>
						</pre>
						<p style="font-size: small;margin-bottom: 0px;">
							Pedestrian frame crop and saveing annotation in CSV...
						</p>
						<pre style="font-size: small; line-height: normal; margin-top: 0px;">
							<code class="hljs python" data-trim contenteditable>
# save data in csv
import pandas as pd
detect_info_df=pd.DataFrame(detect_info_mx, columns=['FRAME', 'FRAME_FILE_PATH', 'CROP_FILE_PATH', 'CROP_NUM', 'xA', 'yA', 'xB', 'yB', 'ID'])
detect_info_df.to_csv(output_info_path + '/detect_info.csv')
							</code>
						</pre>
						<p style="font-size: small;margin-bottom: 0px;">
							Some light optimization...
						</p>
						<pre style="font-size: small; line-height: normal; margin-top: 0px;">
							<code class="hljs python" data-trim contenteditable>
# - decrese frame rate analising frame 
# - if there is a missing frame the video rebuilder will repeat the previous annotation till a threshold
							</code>
						</pre>
						</div>
						</div>
						<hr style="clear:both"/>
						<p style= "font-size: x-large;">
						<small><a href="http://www.pyimagesearch.com/2015/11/09/pedestrian-detection-opencv/">www.pyimagesearch.com/2015/11/09/pedestrian-detection-opencv</a><br/></small>
						</p>
						
					</section>

					<section id="architecture_reideintification">
						<h6>Re-Identification phase</h6>
						<div>
						<div class="col_left_1_2">
							<p style="font-size:large">A trained network to compare people similarity</p>
							<h6>Strengths</h6>
							<p style="font-size:large">
								1. No tuning required.<br/>
								2. Fast in prediction.
							</p>
							<h6>Weakness</h6>
							<p style="font-size:large">
								1. Requires a lot of training with many example<br/>
								2. The input images have a precide size, so we need to resize detected pedestrian.<br/>
								3. Is not trained to recognize background, so it try to recognize every bounding box detected in previous phase.<br/>
								4. The net assume that all people detected are in the people to recognize set.<br/>
								<br/>
							</p>
						</div>
						
						<div class="col_right_1_2">
							<img src="img/reid_phase.png" ></img>
						</div>
						</div>
						<hr style="clear:both"/>
						<p style= "font-size: x-large;">
						<!--
						<small><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Ahmed_An_Improved_Deep_2015_CVPR_paper.pdf">www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Ahmed_An_Improved_Deep_2015_CVPR_paper.pdf</a><br/>
						<a href="https://github.com/Ning-Ding/Implementation-CVPR2015-CNN-for-ReID">github.com/Ning-Ding/Implementation-CVPR2015-CNN-for-ReID</a>
						</small>
						-->
						</p>
					</section>
					
					<section id="architecture_siam_net">
						<h6>Siamese Network</h6>
						<div>
						<div class="col_left_1_2" style="font-size: large;">
							<p>Tied Convolution</p>
							<p>Cross-Input Neighborhood Differences</p>
							<p>Patch Summary Features</p>
							<p>Across-Patch Features</p>
							<p>Higher-Order Relationships</p>
						</div>
						
						<div class="col_right_2_2">
							<img src="img/an_siamese_net_reid.jpg" width="450px"></img>
						</div>
						</div>
						<hr style="clear:both"/>
						<p style= "font-size: x-large;">
						<small><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Ahmed_An_Improved_Deep_2015_CVPR_paper.pdf">www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Ahmed_An_Improved_Deep_2015_CVPR_paper.pdf</a><br/>
						<a href="https://github.com/Ning-Ding/Implementation-CVPR2015-CNN-for-ReID">github.com/Ning-Ding/Implementation-CVPR2015-CNN-for-ReID</a><br/>
						ALT + CLICK -> ZOOM
						</small>
						</p>
					</section>
					
					<section id="architecture_siam_net_detail">
						<h6>Siamese Network detail</h6>
						<img src="img/siamese_net_visualiz.png" width="650px"></img>
						<p style= "font-size: large;">Initial layers learn image features that are important to
distinguish between a positive and a negative pair. Deeper layers learn relationships across the two views so that classification
performance is maximized.</p>
						<br>
					</section>
					
					<section id="architecture_siam_net_train">
						<h6>How I trainied the Siamese Network</h6>
						<img src="img/dataset_market-1501.jpg" height="200px"></img>
						<div>
							<div class="col_left_1_2" style="font-size: large;">
								<p style="font-size: large;">
								<a href="http://www.liangzheng.org/Project/project_reid.html">Market-1501 Dataset</a><br/><br/>
								- Collected in front of a supermarket in Tsinghua University. <br/>
								- A total of six cameras including 5 high-resolution cameras, and one low-resolution camera.<br/>
								- Contains 32,668 annotated bounding boxes of 1,501 identities.<br/>
								- Annotated identity is present in at least two cameras.
								</p>
							</div>
							
							<div class="col_right_2_2">
								<p style="font-size: medium;">
								<B>TRAINING AND TEST PERFORMACES</B><br/><br/>
								optimization method: Stochastic gradient descent<br/>
								epoch train: 5000 | batch size: 64 imgs<br> 
								accuracy: 0.8741 | loss: 0.3382 | time: ~ 48h<br/><br/>
								Laptop:<br/>
								2.5GHz Dual-core Intel i5, 8GB of 1600MHz DDR3 SDRAM, Intel HD Graphics 4000
								<br/>
								<br/>> UBUNTU 16.04 | > PYTHON 3.6.1 <br/>> KERAS 2.0.7 | > TENSORFLOW 1.3.0<br/>
								</p>
							</div>
						</div>
						
						<!--
						<table style="font-size: large;width: 560px;">
						<tbody>
							<tr>
								<td>
									<img src="img/market-1501_dataset.jpeg" width="50px" style="float:left; margin:5px;margin-top: 20px"></img>
									<p style="font-size: medium;">
									<B>Market-1501 Dataset</B><br/>
									<a href="http://www.liangzheng.org/Project/project_reid.html">www.liangzheng.org/.../project_reid.htm</a><br/><br/>
									epoch train: 5000<br/> accuracy: 0.8741, loss: 0.3382<br/>
									</p>
								</td>
							</tr>
							<tr>
						
								<td>
									<img src="img/eg_viper.png" width="50px" style="float:left; margin:5px;margin-top: 20px"></img>
									<p style="font-size: large;">
									<B>Viper (Viewpoint Invariant Ped. Rec.)</B><br/>
									<a href="https://vision.soe.ucsc.edu/node/178">vision.soe.ucsc.edu/node/178</a><br/><br/>
									epoch: [TODO]<br/> accuracy: [TODO], loss [TODO]<br/>
									</p>
								</td>
							</tr>
							<tr>
								<td>
									<img src="img/eg_CUHK03_detected.png" width="50px" style="float:left; margin:5px;margin-top: 20px"></img>
									<p style="font-size: large;">
									<B>CUHK01/CUHK02/CUHK03</B><br/>
									<a href="http://www.ee.cuhk.edu.hk/~rzhao/">www.ee.cuhk.edu.hk/~rzhao</a><br/><br/>
									epoch: [TODO]<br/> accuracy: [TODO], loss [TODO]<br/>
									</p>
								</td>
							</tr>
						</tbody>
						</table>
						
						-->
						
						<hr style="clear:both"/>
						<p style= "font-size: x-large;">
						<small>
							<a href="http://robustsystems.coe.neu.edu/sites/robustsystems.coe.neu.edu/files/systems/projectpages/reiddataset.html">robustsystems.coe.neu.edu/sites/robustsystems.coe.neu.edu/files/systems/projectpages/reiddataset.html</a><br/>
						</small>
						</p>
					</section>
					
				</section>

				<section id="testing">
					
					<section id="testing_overview">
						<h6>Testing.... </h6>
						<div>
						<div class="col_left_1_3" style="font-size: large;">
							
							<h6>Test detection</h6>
							<p>
								<img src="img/accuracy_dete.JPG" width="240px"></img><br/>
								for every frame put in relation <i>detected boundig box</i> with <i>boundig box annotated</i> manually
							</p>
							<h6>Test reidentification</h6>
							<p>
								<img src="img/accuracy_reid.JPG" width="240px"></img><br/>
								verify if identity is correct for every detected pedestian in pevious phase with annotated ID.
							</p>
							
						</div>
						
						<div class="col_right_2_3">
							<table>
							<tbody style="font-size: large;">
								<tr>
								
									<td>
										<img src="img/cvpr10_track2d_thumb.png" width="100px" style="float:left; margin:5px"></img>
										<p style="margin-top: 5px;">
										<B>TUD Multiview Pedestrians</B><br/>
										<a href="https://www.d2.mpi-inf.mpg.de/node/428">www.d2.mpi-inf.mpg.de/node/428</a><br/>
										Up to 10 people in the same frame, video in outdoor, low brightness conditions, some overlapping people.
										<br/>
										</p>
									</td>
								</tr>
								
								<tr>
									<td>
										<img src="img/PETS-S2L1.jpg" width="100px" style="float:left; margin:5px"></img>
										<p style="margin-top: 5px;"><B>PETS 2009</B><br/>
										<a href="http://www.cvg.reading.ac.uk/PETS2009/">www.cvg.reading.ac.uk/PETS2009</a><br/>
										Crowd situation, video in outdoor and high brightness conditions.
										</p>
								</tr>
								
								<tr>
									<td>
										<img src="img/3d_pes.jpg" width="100px" style="float:left; margin:5px"></img>
										<p style="margin-top: 5px;"><B>3DPeS (3D People Surveillance Dataset)</B> is a surveillance dataset<br/>
										<a href="http://www.openvisor.org/3dpes.asp">www.openvisor.org/3dpes.asp</a><br/>
										Up 7 people in the same frame, video in outdoor, high brightness conditions.
										</p>
								</tr>
								
								<tr>
									<td>
										<img src="img/own_video_frame.jpg" width="100px" style="float:left; margin:5px"></img>
										<p style="margin-top: 5px;"><B>MY DATASET (@ UNIMORE)</B><br/>
										<a href="https://www.dropbox.com/sh/9njwblsl12ihzou/AAC0qW74RN17IP456rTIs7o0a?dl=0">My dropbox</a><br/>
										2 people both in static and moving camera in outdoor, high brightness conditions. 
										</p>
									</td>
								</tr>
								
							</tbody>
							</table>
						</div>
						</div>
						
						<hr style="clear:both"/>
						<p style= "font-size: large;">
						<small>
							<a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Ahmed_An_Improved_Deep_2015_CVPR_paper.pdf">www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Ahmed_An_Improved_Deep_2015_CVPR_paper.pdf</a><br/>
							<a href="https://github.com/francescosolera/TBD-evaluation/blob/master/README.md">github.com/francescosolera/TBD-evaluation</a><br/>
							<a href="http://www.milanton.de/data.html">http://www.milanton.de/data.html</a>
						</small>
						</p>
					</section>
					
					<section id="testing_overview_01">
						<h6>TUD Multiview Pedestrians</h6>
						<div>
						<div class="col_left_1_2" style="font-size: large;">
							<img src="img/cvpr10_tud_stadtmitte_000.jpg" width="380px" style=" margin:10px"></img>
							
						</div>
						<div class="col_right_1_2" style="font-size: large;text-align:left;">
							<p>
								<a href="https://www.d2.mpi-inf.mpg.de/node/428">www.d2.mpi-inf.mpg.de/node/428</a><br/>
								The TUD Pedestrians dataset from Micha Andriluka, Stefan Roth and Bernt Schiele [AndrilukaCVPR2008].<br/>
								It consists of 250 images with 311 fully visible people with significant variation in clothing and articulation.<br/>
								In this dataset dataset there are  3 video.
								Usually it was used in the project to evaluate single-frame people detection.<br/>
							</p>
						</div>
						</div>
						
						<div style="clear:both;margin-top:5px" >
						<div class="col_left_1_2" style="font-size: large;">
							<p style="margin-top:5px">
								<B>Accuracy detection: 0,634948097</B><br/>
								<br/>
								Tot annotated detection 1157 smpl.<br/><br/>
								Some relevanti info in **CONFUSION MATRIX**<br/>
								TP (correct detection):	734 smpl.<br/>
								FN (missing detection):	422 smpl.<br/>
								FP (ghost detection):	0 smpl.<br/>
							</p>
						</div>
						<div class="col_right_1_2" style="font-size: large;text-align:left;">
							<p style="margin-top:5px">
								<B>Accuracy reidentification:</B><br/><br/>
								
								Tot. reidentification done: 728<br/>
								true match: 0.510989 (372 smpl.)<br/> 
								impossible match: 0.337912 (246 smpl.)**<br/> 
								false match:  0.151099 (110 smpl.)<br/> <br/>
								
								** people to reidentify are less than people in videos<br/>
								if we don't care impossible match accuracy is 0.771784<br/>
								With a threshold like 0.7  the accuracy grow little (~0.69)
							</p>
						</div>
						</div>
						
					</section>
					
					<section id="testing_overview_02">
						<h6>> MY DATASET (@ UNIMORE) <</h6>
						<div>
						<div class="col_left_1_2" style="font-size: large;">
							<img src="img/own_video_frame.jpg" width="400px" style="float:left; margin:5px"></img>
						</div>
						<div class="col_right_1_2" style="font-size: large;text-align:left;">
							<p>
								<a href="https://www.dropbox.com/sh/9njwblsl12ihzou/AAC0qW74RN17IP456rTIs7o0a?dl=0">My dataset at unimore available in dropbox</a><br/>
								Data acquired from the Axis camera mounted at UNIMORE, with video, 
								people and situation semi-constrained and controlled, usually there are 2 people 
								video are acquire both in static and moving camera in outdoor in high brightness conditions. 
								Videos are created with the msmpeg4v2 codec (from ffmpeg) 
							</p>
						</div>
						</div>
						<div style="clear:both;margin-top:5px" >
						<div class="col_left_1_2" style="font-size: large;">
							<p style="margin-top:5px">
								<B>Accuracy detection: 0,619479049</B><br/>
								<br/>
								Tot annotated detection 883 smpl.<br/><br/>
								Some relevanti info in **CONFUSION MATRIX**<br/>
								TP (correct detection):	547 smpl.<br/>
								FN (missing detection):	336 smpl.<br/>
								FP (ghost detection):	0 smpl.<br/>
							</p>
						</div>
						<div class="col_right_1_2" style="font-size: large;text-align:left;">
							<p style="margin-top:5px">
								<B>Accuracy reidentification:</B><br/><br/>
							
								Tot. reidentification done: 543<br>
								True match: 0.732965 (398 smpl.)<br>
								False match: 0.267035 (145 smpl.)<br><br>

								The video contains only people to reidentify.
							</p>
						</div>
						</div>
					</section>
					
				</section>

				<section id="conclusion">
					<h6>Conclusion</h6>
					
					<table>
						<tbody>
							<tr>
								<td>Some missing in detection phase, need more tuning</td>
							</tr>
							<tr>
								<td>Not suitable for realtime application, but good approach for batch services</td>
							</tr>
							<tr>
								<td>Training performaces can be improved</td>
							</tr>
						</tbody>
					</table>
				</section>
				
				<section id="future_works">
					<h6>Future works</h6>
					<ul>
						<li>Create a service API to make project available to third party</li>
						<li>Introduct a classificator that distiguish from background and not background to eliminate false positive</li>
						<li>Porting siamese net in CAFFE and THEANO to compare speed and accuracy performances in training and prediction</li>
					</ul>
				</section>
				
				<section id="tech_resources">
					<h6>Tech Resouces</h6>
					<p style= "font-size: x-large;">
						<small>
						<a href="http://lab.hakim.se/reveal-js">lab.hakim.se/reveal-js [Presentation] </a><br/>
						<a href="https://c9.io/">c9.io [online IDE]</a><br/>
						<a href="https://www.flaticon.com/free-icon/">www.flaticon.com/free-icon/</a><br/>
						
						<a href="https://www.pyimagesearch.com/2014/11/10/histogram-oriented-gradients-object-detection/">histogram-oriented-gradients-object-detection</a>
						</small>
					</p>
				</section>

			</div>

		</div>

		<script src="js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>

			// More info https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				controls: true,
				progress: true,
				history: true,
				center: true,

				transition: 'slide', // none/fade/slide/convex/concave/zoom

				// More info https://github.com/hakimel/reveal.js#dependencies
				dependencies: [
					{ src: 'js/classList.js', condition: function() { return !document.body.classList; } },
					{ src: 'js/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'js/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'js/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
					{ src: 'js/zoom-js/zoom.js', async: true },
					{ src: 'js/notes/notes.js', async: true }
				]
			});

		</script>

	</body>
</html>
